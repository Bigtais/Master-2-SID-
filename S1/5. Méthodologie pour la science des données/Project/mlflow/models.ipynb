{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train' from 'c:\\\\Users\\\\aghil\\\\OneDrive\\\\Bureau\\\\PredectiveMassMLOps\\\\mlflow\\\\../scripts\\\\train.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import importlib\n",
    "import preprocess_data, visualise, train, evaluate\n",
    "\n",
    "importlib.reload(preprocess_data)\n",
    "importlib.reload(visualise)\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_data import *\n",
    "from visualise import *\n",
    "from evaluate import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/raw/data_Mass.xlsx'  \n",
    "df = load_dataset(file_path)\n",
    "\n",
    "df = drop_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sen1', 'sen2', 'sen3', 'sen4', 'sen5', 'sen6', 'sen7', 'sen8', 'sen9', 'sen10', 'sen11', 'sen12', 'sen13', 'sen14', 'sen15']\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = split_data(df, features, 0.2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model_lr = train_lr(X_train, y_train, \"../models/lr.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  3917.3760937481557\n",
      "R2 Score :  0.9130703668471682\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test\n",
    "print(\"Mean Squared Error : \", evaluate(model_lr, X_test, y_test, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_lr, X_test, y_test, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  11875.311203180707\n",
      "R2 Score :  0.6995013763252966\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on val\n",
    "print(\"Mean Absolute Error : \", evaluate(model_lr, X_val, y_val, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_lr, X_val, y_val, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log model to mlflow\n",
    "with mlflow.start_run(run_name=\"LinearRegressor\"):\n",
    "    mlflow.log_params(model_lr.get_params())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    r2_lr_test = evaluate(model_lr, X_test, y_test, r2_score)\n",
    "    mse_lr_test = evaluate(model_lr, X_test, y_test, mean_squared_error)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    r2_lr_val = evaluate(model_lr, X_val, y_val, r2_score)\n",
    "    mse_lr_val = evaluate(model_lr, X_val, y_val, mean_squared_error)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"R2_Test\", r2_lr_test)\n",
    "    mlflow.log_metric(\"MSE_Test\", mse_lr_test)\n",
    "\n",
    "    # Log metrics for validation set\n",
    "    mlflow.log_metric(\"R2_Val\", r2_lr_val)\n",
    "    mlflow.log_metric(\"MSE_Val\", mse_lr_val)\n",
    "\n",
    "    mlflow.sklearn.log_model(model_lr, \"Linear Regressor\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid to search\n",
    "param = {\n",
    "    'n_estimators': [50, 100, 150]\n",
    "    # 'learning_rate': [0.01, 0.1, 0.2],\n",
    "    # 'max_depth': [3, 4, 5],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4]\n",
    "    # 'subsample': [0.8, 1.0],\n",
    "    # 'max_features': [None, 'sqrt', 'log2'],\n",
    "    # 'alpha': [0.9, 0.7, 0.5],\n",
    "    # 'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "# Create and train the model\n",
    "model_gbr = train_gradient_boosting_regressor(X_train, y_train, param, \"../models/gbr.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  5711.935222219343\n",
      "R2 Score :  0.8732476990777829\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test\n",
    "print(\"Mean Absolute Error : \", evaluate(model_gbr, X_test, y_test, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_gbr, X_test, y_test, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  2706.4217938914776\n",
      "R2 Score :  0.9315153927141058\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on val\n",
    "print(\"Mean Absolute Error : \", evaluate(model_gbr, X_val, y_val, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_gbr, X_val, y_val, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Log model to mlflow\n",
    "with mlflow.start_run(run_name=\"GradientBoostingRegressor\"):\n",
    "    mlflow.log_params(model_gbr.get_params())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    r2_gbr_test = evaluate(model_gbr, X_test, y_test, r2_score)\n",
    "    mse_gbr_test = evaluate(model_gbr, X_test, y_test, mean_squared_error)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    r2_gbr_val = evaluate(model_gbr, X_val, y_val, r2_score)\n",
    "    mse_gbr_val = evaluate(model_gbr, X_val, y_val, mean_squared_error)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"R2_Test\", r2_gbr_test)\n",
    "    mlflow.log_metric(\"MSE_Test\", mse_gbr_test)\n",
    "\n",
    "    # Log metrics for validation set\n",
    "    mlflow.log_metric(\"R2_Val\", r2_gbr_val)\n",
    "    mlflow.log_metric(\"MSE_Val\", mse_gbr_val)\n",
    "\n",
    "    mlflow.sklearn.log_model(model_gbr, \"Gradient Boosting Regressor\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Xgboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters n_estimators and maxdepth \n",
    "param = [50,5]\n",
    "\n",
    "#create and train the model \n",
    "model_xgbr = train_xgboost(X_train, y_train,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  8571.5332770257\n",
      "R2 Score :  0.8097909862373719\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test\n",
    "print(\"Mean Absolute Error : \", evaluate(model_xgbr, X_test, y_test, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_xgbr, X_test, y_test, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  1159.830313946586\n",
      "R2 Score :  0.9706510922472669\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on val\n",
    "print(\"Mean Absolute Error : \", evaluate(model_xgbr, X_val, y_val, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_xgbr, X_val, y_val, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Log model to mlflow\n",
    "with mlflow.start_run(run_name=\"XgboostRegressor\"):\n",
    "    mlflow.log_params(model_xgbr.get_params())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    r2_xgbr_test = evaluate(model_xgbr, X_test, y_test, r2_score)\n",
    "    mse_xgbr_test = evaluate(model_xgbr, X_test, y_test, mean_squared_error)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    r2_xgbr_val = evaluate(model_xgbr, X_val, y_val, r2_score)\n",
    "    mse_xgbr_val = evaluate(model_xgbr, X_val, y_val, mean_squared_error)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"R2_Test\", r2_xgbr_test)\n",
    "    mlflow.log_metric(\"MSE_Test\", mse_xgbr_test)\n",
    "\n",
    "    # Log metrics for validation set\n",
    "    mlflow.log_metric(\"R2_Val\", r2_xgbr_val)\n",
    "    mlflow.log_metric(\"MSE_Val\", mse_xgbr_val)\n",
    "\n",
    "    mlflow.sklearn.log_model(model_xgbr, \"Xgboost Regressor\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid to search\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create and train the model\n",
    "model_rf = train_random_forest_regressor(X_train, y_train, params, file='../models/rf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  8291.666261904766\n",
      "R2 Score :  0.8160014537477209\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test\n",
    "print(\"Mean Absolute Error : \", evaluate(model_rf, X_test, y_test, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_rf, X_test, y_test, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error :  5683.426349999998\n",
      "R2 Score :  0.8561838282205096\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on val\n",
    "print(\"Mean Absolute Error : \", evaluate(model_rf, X_val, y_val, mean_squared_error))\n",
    "print(\"R2 Score : \", evaluate(model_rf, X_val, y_val, r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aghil\\anaconda3\\envs\\Tp1\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Log model to mlflow\n",
    "with mlflow.start_run(run_name=\"RandomForestRegressor\"):\n",
    "    mlflow.log_params(model_rf.get_params())\n",
    "\n",
    "    # Evaluate on test set\n",
    "    r2_rf_test = evaluate(model_rf, X_test, y_test, r2_score)\n",
    "    mse_rf_test = evaluate(model_rf, X_test, y_test, mean_squared_error)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    r2_rf_val = evaluate(model_rf, X_val, y_val, r2_score)\n",
    "    mse_rf_val = evaluate(model_rf, X_val, y_val, mean_squared_error)\n",
    "\n",
    "    # Log metrics for test set\n",
    "    mlflow.log_metric(\"R2_Test\", r2_rf_test)\n",
    "    mlflow.log_metric(\"MSE_Test\", mse_rf_test)\n",
    "\n",
    "    # Log metrics for validation set\n",
    "    mlflow.log_metric(\"R2_Val\", r2_rf_val)\n",
    "    mlflow.log_metric(\"MSE_Val\", mse_rf_val)\n",
    "\n",
    "    mlflow.sklearn.log_model(model_rf, \"RandomForestRegressor\")\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
